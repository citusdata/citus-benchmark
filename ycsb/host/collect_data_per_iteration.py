# Collects postgresql logs from /dat/14/data/pg_log/ produced by user ‘monitor’ on every worker/coordinator
# Stores all collected files in RDS and S3 so that they persist
# If succeeded, delete locally generated files and folders (optional)

import os
from helper import *
import yaml
from logs import Logging
import sys

homedir = os.getcwd()
bucket = sys.argv[1]
iteration = sys.argv[2]

# read config file
with open('config.yml', 'r') as f:
    try:
        config = yaml.safe_load(f)
        ycsb = config['ycsb']
        cluster = config['cluster']
    except yaml.YAMLError as exc:
        print(exc)


# Create a logging instance
logs = Logging(iteration = iteration, resource = cluster['resource'], prefix = cluster['prefix'], host = cluster['host'], password = cluster['pgpassword'],
port = cluster['port'], shard_count = ycsb['shard_count'])

# Collect cpu usage from worker nodes
logs.collect_iostat()

#  Delete nohup.out generated by iostat
logs.delete_iostat()

#  Get raw postgresql data from worker nodes
logs.get_postgresql()

# Runs script that pushes gathered data to Blob storage and a PostgreSQL DB
# Push to postgresql
os.chdir(homedir + "/storage")
path = homedir + f"/logs/scripts/{cluster['resource']}"

# Push postgresql and IOSTAT data to blob
run(["./push-to-blob.sh", f"{path}/pglogs/", bucket, f"{cluster['resource']}/pglogs"], shell = False)
run(["./push-to-blob.sh", f"{path}/general/", bucket, f"{cluster['resource']}/general"], shell = False)

print(f"Iteration {iteration} finished")



